{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706edba9-eb08-4f57-bcc5-a1bb7024f068",
   "metadata": {},
   "source": [
    "# Project Final Report\n",
    "\n",
    "### Due: Midnight on April 27 (2-hour grace period) — 50 points  \n",
    "\n",
    "### No late submissions will be accepted.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Your final submission consists of **three components**:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Final Report Notebook [40 pts]\n",
    "\n",
    "Complete all sections of this notebook to document your final decisions, results, and broader context.\n",
    "\n",
    "- **Part A**: Select the single best model from your Milestone 2 experiments. Now that you’ve finalized your model, revisit your decisions from Milestones 1 and 2. Are there any steps you would change—such as cleaning, feature engineering, or model evaluation—given what you now know?\n",
    "\n",
    "- **Part B**: Write a technical report following standard conventions, for example:\n",
    "  - [CMU guide to structure](https://www.stat.cmu.edu/~brian/701/notes/paper-structure.pdf)\n",
    "  - [Data science report example](https://www.projectpro.io/article/data-science-project-report/620)\n",
    "  - The Checklist given in this week's Blackboard Lesson (essentially the same as in HOML).\n",
    "    \n",
    "  Your audience here is technically literate but unfamiliar with your work—like your manager or other data scientists. Be clear, precise, and include both code (for illustration), charts/plots/illustrations, and explanation of what you discovered and your reasoning process. \n",
    "\n",
    "The idea here is that Part A would be a repository of the most important code, for further work to come, and Part B is\n",
    "the technical report which summarizes your project for the data science group at your company. Do NOT assume that readers of Part B are intimately familiar with Part A; provide code for illustration as needed, but not to run.\n",
    "\n",
    "Submit this notebook as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. PowerPoint Presentation [10 pts]\n",
    "\n",
    "Create a 10–15 minute presentation designed for a general audience (e.g., sales or marketing team).\n",
    "\n",
    "- Prepare 8–12 slides, following the general outline of the sections of Part B. \n",
    "- Focus on storytelling, visuals (plots and illustrations), and clear, simplified language. No code!\n",
    "- Use any presentation tool you like, but upload a PDF version.\n",
    "- List all team members on the first slide.\n",
    "\n",
    "Submit as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Individual Assessment\n",
    "\n",
    "Each team member must complete the Individual Assessment Form (same as in Milestone 1), sign it, and upload it via their own Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "-  Final Report Notebook — Team leader submission\n",
    "-  PDF Slides — Team leader submission\n",
    "-  Individual Assessment Form — Each member submits their own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee15ea-7cf4-4d57-b83a-18f186f0d204",
   "metadata": {},
   "source": [
    "## Part A: Final Model and Design Reassessment [10 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model and revisit earlier decisions to determine if any should be revised in light of your complete modeling workflow. You’ll also consolidate and present the key code used to run your model on the preprocessed dataset, with thoughtful documentation of your reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Reconsider **at least one decision from Milestone 1** (e.g., preprocessing, feature engineering, or encoding). Explain whether you would keep or revise that decision now that you know which model performs best. Justify your reasoning.\n",
    "  \n",
    "- Reconsider **at least one decision from Milestone 2** (e.g., model evaluation, cross-validation strategy, or feature selection). Again, explain whether you would keep or revise your original decision, and why.\n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset. This section should be a clean, readable summary of the most important steps from Milestones 1 and 2, adapted as needed to fit your final model choice and your reconsiderations as just described. \n",
    "\n",
    "- Use Markdown cells and inline comments to explain the structure of the code clearly but concisely. The goal is to make your reasoning and process easy to follow for instructors and reviewers.\n",
    "\n",
    "> Remember: You are not required to change your earlier choices, but you *are* required to reflect on them and justify your final decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0491a",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be0244-e449-4322-a036-d188724901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Useful Imports\n",
    "# =============================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Kaggle and Progress Tracking\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f9fff",
   "metadata": {},
   "source": [
    "## Load Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ffeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, target_column=None, random_state=random_state):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # check if the dataset has been loaded properly\n",
    "    if df.empty:\n",
    "        raise ValueError(\"the dataset is empty, please check the path or the file content.\")\n",
    "    print(f\"loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "    # Split the data into features and target variables\n",
    "    X = df.drop(columns=target_column)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # create scaled versions of the train and test features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    # Convert scaled data back to DataFrame for better readability\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from github and preprocess the data\n",
    "X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, df = load_data(\n",
    "    'https://raw.githubusercontent.com/ys1433/Module-3-Assignments/refs/heads/main/zillow_cleaned.csv',\n",
    "    target_column='taxvaluedollarcnt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365e849",
   "metadata": {},
   "source": [
    "## Run and Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, x_train, y_train, return_model=False, n_repeats=5, n_jobs=-1, random_state=random_state, **model_params):\n",
    "\n",
    "    # Instantiate the model if a class is provided, so for example can use either BaggingRegressor or BaggingRegressor() as argument. \n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "\n",
    "    neg_rmse_scores = cross_val_score(model, x_train, y_train, scoring = 'neg_root_mean_squared_error',\n",
    "                                     cv = RepeatedKFold(n_splits=5, n_repeats=n_repeats, random_state=random_state), n_jobs = n_jobs)\n",
    "    \n",
    "    mean_cv_rmse = -np.mean(neg_rmse_scores)\n",
    "    std_cv_rmse  = np.std(neg_rmse_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Compute training RMSE\n",
    "    if return_model:\n",
    "        train_preds = model.predict(x_train)\n",
    "        train_rmse = root_mean_squared_error(y_train, train_preds)\n",
    "        \n",
    "        return mean_cv_rmse, std_cv_rmse, train_rmse, model\n",
    "    \n",
    "    else:\n",
    "        train_preds = model.predict(x_train)\n",
    "        train_rmse = root_mean_squared_error(y_train, train_preds)\n",
    "\n",
    "        return mean_cv_rmse, std_cv_rmse, train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c628a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    models, \n",
    "    x_train, y_train,\n",
    "    return_model=False,\n",
    "    random_state=random_state, \n",
    "    **model_params\n",
    "):\n",
    "\n",
    "    results = {}\n",
    "    fitted_models = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"Evaluating {name}…\")\n",
    "\n",
    "        if return_model:\n",
    "            # run_model must accept x_test, y_test, test=True\n",
    "            mean_cv_rmse, std_cv_rmse, train_rmse, fitted_model = run_model(\n",
    "                model,\n",
    "                x_train, y_train,\n",
    "                return_model=True,\n",
    "                random_state=random_state,\n",
    "                **model_params\n",
    "            )\n",
    "            fitted_models[name] = fitted_model\n",
    "        else:\n",
    "            # only get the three training‐set metrics\n",
    "            mean_cv_rmse, std_cv_rmse, train_rmse = run_model(\n",
    "                model,\n",
    "                x_train, y_train,\n",
    "                return_model=False,\n",
    "                random_state=random_state,\n",
    "                **model_params\n",
    "            )\n",
    "        # now format only the things you actually have\n",
    "        row = {\n",
    "            'Mean CV RMSE': dollar_format(mean_cv_rmse, 2),\n",
    "            'STD CV RMSE' : dollar_format(std_cv_rmse, 2),\n",
    "            'Train RMSE': dollar_format(train_rmse, 2),\n",
    "        }\n",
    "\n",
    "        results[name] = row\n",
    "\n",
    "    df = pd.DataFrame(results).T\n",
    "    df.index.name = 'Model'\n",
    "\n",
    "    if return_model:\n",
    "        return df.sort_values(by='Mean CV RMSE'), fitted_models\n",
    "    else:\n",
    "        return df.sort_values(by='Mean CV RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419f5c2",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322538b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square footage features have large values and are highly skewed, consider log transformation\n",
    "\n",
    "# basementsqft, fireplacecnt, garagecarcnt, poolcnt have a lot of zeros (> 75%)), consider transforming them to binary features\n",
    "zero_list = [\n",
    "    'basementsqft',\n",
    "    'fireplacecnt',\n",
    "    'garagecarcnt',\n",
    "]\n",
    "# Check the percentage of zeros in the zero_list features\n",
    "for col in zero_list:\n",
    "    zero_percentage = (df[col] == 0).mean() * 100\n",
    "    print(f\"{col}: {zero_percentage:.2f}% zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transfermation of square footage features\n",
    "\n",
    "# define log transformation function\n",
    "def log_transformation(df, column):\n",
    "  df[f'{column}_log_transformed'] = np.log(df[column])\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(8,2))\n",
    "  axes[0].boxplot(df[column])\n",
    "  axes[0].set_title(column)\n",
    "  axes[1].boxplot(df[f'{column}_log_transformed'])\n",
    "  axes[1].set_title(f'{column}_log')\n",
    "\n",
    "# apply log transformation to selected columns with large values and high skewness\n",
    "# (all square footage columns except for basementsqft as it has only 38 non-zero rows)\n",
    "X_train_log = X_train.copy()\n",
    "log_list = ['calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'lotsizesquarefeet']\n",
    "\n",
    "for f in log_list:\n",
    "    log_transformation(X_train_log, f)\n",
    "\n",
    "X_train_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfba8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial transformation\n",
    "\n",
    "# define polynomial transformation function\n",
    "def polynomial_transformation(df, column, degree):\n",
    "  df[f'{column}_poly_transformed'] = df[column]**degree\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(8,2))\n",
    "  axes[0].boxplot(df[column])\n",
    "  axes[0].set_title(column)\n",
    "  axes[1].boxplot(df[f'{column}_poly_transformed'])\n",
    "  axes[1].set_title(f'{column}_poly')\n",
    "\n",
    "# apply polynomial transformation to selected columns\n",
    "X_train_poly = X_train_log.copy()\n",
    "poly_list = ['calculatedbathnbr']\n",
    "for f in poly_list:\n",
    "  polynomial_transformation(X_train_poly, f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ce4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder for Categorical features\n",
    "\n",
    "# define one-hot encoding function\n",
    "def one_hot_encode(df, columns):\n",
    "    df = df.copy()\n",
    "    df[columns] = df[columns].astype(str)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded = encoder.fit_transform(df[columns])\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded,\n",
    "        columns=encoder.get_feature_names_out(columns),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# apply one-hot encoding to selected categorical features (id type features without too many unique values)\n",
    "encode_list = ['regionidcounty', 'heatingorsystemtypeid', 'propertylandusetypeid' ]\n",
    "X_train_encoded = one_hot_encode(X_train_poly, encode_list)\n",
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27868a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert zero_list features to binary features\n",
    "def convert_to_binary(df, column):\n",
    "        df[f'{column}_binary'] = (df[column] != 0).astype(int)\n",
    "\n",
    "X_train_binary = X_train_encoded.copy()\n",
    "print(zero_list)\n",
    "for f in zero_list:\n",
    "    convert_to_binary(X_train_binary, f)\n",
    "\n",
    "X_train_binary.shape\n",
    "print(X_train_binary.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2751376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the transformed features\n",
    "scaler = StandardScaler()\n",
    "X_train_transformed_scaled = scaler.fit_transform(X_train_binary)\n",
    "\n",
    "X_train_transformed_scaled = pd.DataFrame(X_train_transformed_scaled, columns = X_train_binary.columns)\n",
    "\n",
    "print(X_train_transformed_scaled.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all transformed features\n",
    "transformed_features = log_list + poly_list + encode_list + zero_list\n",
    "print(len(transformed_features), transformed_features)\n",
    "\n",
    "# create a df without the orginal features\n",
    "X_train_transformed_scaled_original_dropped = X_train_transformed_scaled.drop(columns=transformed_features)\n",
    "X_train_transformed_scaled_original_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8153e5d",
   "metadata": {},
   "source": [
    "## **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdee376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_parameter(model,\n",
    "                    Parameters,\n",
    "                    param,\n",
    "                    parameter_list,\n",
    "                    x_train          = X_train_scaled,\n",
    "                    y_train          = y_train,\n",
    "                    verbose          = True,\n",
    "                    show_rmse        = True,\n",
    "                    n_iter_no_change = None,\n",
    "                    delta            = 0.001):\n",
    "    \n",
    "    start = time.time()\n",
    "    Parameters = Parameters.copy()  # Avoid modifying the original dictionary\n",
    "    \n",
    "    cv_rmses, std_cvs, train_rmses = [], [], []\n",
    "    no_improve_count = 0\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Run over each value in parameter_list\n",
    "    for p in tqdm(parameter_list, desc=f\"Sweeping {param}\"):\n",
    "        Parameters[param] = p\n",
    "        P_temp = Parameters.copy()\n",
    "        # Remove MSE_found if present, just in case\n",
    "        P_temp.pop('RMSE_found', None)\n",
    "        \n",
    "        cv_rmse, std_cv, train_rmse = run_model(\n",
    "            model, x_train, y_train, **P_temp\n",
    "        )\n",
    "        \n",
    "        cv_rmses.append(cv_rmse)\n",
    "        std_cvs.append(std_cv)\n",
    "        train_rmses.append(train_rmse)\n",
    "        \n",
    "        # Early-stopping logic\n",
    "        if cv_rmse < best_rmse - delta:\n",
    "            best_rmse = cv_rmse\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if n_iter_no_change is not None and no_improve_count >= n_iter_no_change:\n",
    "            print(f\"Early stopping: No improvement after {n_iter_no_change} iterations.\")\n",
    "            break\n",
    "    \n",
    "    # Identify best parameter\n",
    "    min_cv_rmse = min(cv_rmses)\n",
    "    min_index = cv_rmses.index(min_cv_rmse)\n",
    "    best_param = parameter_list[min_index]\n",
    "    Parameters[param] = best_param\n",
    "    Parameters['RMSE_found'] = min_cv_rmse\n",
    "    \n",
    "    if verbose:\n",
    "        # Prepare for plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "        \n",
    "        # We only need as many parameter values as we actually computed\n",
    "        partial_param_list = parameter_list[:len(cv_rmses)]\n",
    "        \n",
    "        # Check if our parameter list is Boolean so we can label accordingly\n",
    "        is_boolean = all(isinstance(val, bool) for val in partial_param_list)\n",
    "        if is_boolean:\n",
    "            # Convert booleans to integer indices for plotting\n",
    "            x_vals = list(range(len(partial_param_list)))\n",
    "            x_labels = [str(val) for val in partial_param_list]\n",
    "        else:\n",
    "            # Treat numeric or other types as-is\n",
    "            x_vals = partial_param_list\n",
    "            x_labels = partial_param_list\n",
    "        \n",
    "        error_name = 'RMSE' if show_rmse else 'MSE'\n",
    "        \n",
    "        # ----- First plot: (R)MSE -----\n",
    "        ax1.set_title(f\"{error_name} vs {param}\")\n",
    "        \n",
    "        # Apply dollar formatting ONLY if we're showing RMSE\n",
    "        if show_rmse:\n",
    "            ax1.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
    "        \n",
    "        # Plot lines\n",
    "        ax1.plot(x_vals,\n",
    "                 cv_rmses,\n",
    "                 marker='.', label=f\"CV {error_name}\", color='blue')\n",
    "        ax1.plot(x_vals,\n",
    "                 train_rmses,\n",
    "                 marker='.', label=f\"Train {error_name}\", color='green')\n",
    "        ax1.scatter([x_vals[min_index]],\n",
    "                    [min_cv_rmse],\n",
    "                    marker='x', label=f\"Best CV {error_name}\", color='red')\n",
    "        \n",
    "        ax1.set_ylabel(error_name)\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "        \n",
    "        # ----- Second plot: CV Std Dev -----\n",
    "        ax2.set_title(f\"CV Standard Deviation vs {param}\")\n",
    "        ax2.plot(x_vals, std_cvs, marker='.', label=f\"CV {error_name} Std\", color='blue')\n",
    "        ax2.set_xlabel(param)\n",
    "        ax2.set_ylabel(\"Standard Deviation\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(alpha=0.5)\n",
    "        \n",
    "        # If we are using boolean x-values, set custom ticks\n",
    "        if is_boolean:\n",
    "            ax2.set_xticks(x_vals)\n",
    "            ax2.set_xticklabels(x_labels)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Execution Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end - start)))\n",
    "    \n",
    "    return Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Default_Parameters_GradientBoosting = {\n",
    "    'learning_rate'           : 0.1,             # Shrinks the contribution of each tree. Affects the speed of learning and overfitting.\n",
    "    'n_estimators'            : 100,             # The number of boosting stages to be run. More estimators can improve performance but increase training time.\n",
    "    'max_depth'               : 3,               # Maximum depth of individual trees. Controls model complexity.\n",
    "    'max_features'            : None,            # Number of features to consider when looking for best split. Can help reduce overfitting.\n",
    "    'random_state'            : 42,              # Controls randomness of boosting. Useful for reproducibility.\n",
    "    'RMSE_found'               : float('inf')     # NOT a parameter, but will record the MSE found for the current parameter choices\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameters_GB = Default_Parameters_GradientBoosting.copy()\n",
    "Parameters_GB_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b5ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_and_ranges_gb = [\n",
    "    ('learning_rate', np.linspace(0.1, 0.3, 10)),\n",
    "    ('n_estimators', range(100, 200, 10)),\n",
    "    ('max_depth', range(3, 12, 1)),\n",
    "    ('max_features', np.linspace(0.1, 1.0, 10)) \n",
    "]\n",
    "\n",
    "for param, parameter_list in parameters_and_ranges_gb:\n",
    "    Parameters_GB = sweep_parameter(GradientBoostingRegressor,\n",
    "                                Parameters_GB,\n",
    "                                param,\n",
    "                                parameter_list,\n",
    "                                x_train  = X_train_scaled,\n",
    "                                y_train  = y_train\n",
    "                                )\n",
    "\n",
    "    print(f'\\nParameter {param} = {Parameters_GB[param]}; RMSE = {dollar_format(Parameters_GB['RMSE_found'],2)}\\n')\n",
    "    print(Parameters_GB_list)\n",
    "\n",
    "Parameters_GB_list.append(Parameters_GB)\n",
    "pd.DataFrame(Parameters_GB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67717482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the parameters for the best gradient boosting model\n",
    "best_gb_params = Parameters_GB_list[0]\n",
    "\n",
    "# Evaluate the best gradient boosting model\n",
    "gb_evaluation = evaluate_model({\"Gradient Boosting\": GradientBoostingRegressor()}, \n",
    "                               X_train_transformed_scaled_original_dropped, y_train, **best_gb_params)\n",
    "gb_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce548ba-c1ce-4a66-8885-9f7e40b1c404",
   "metadata": {},
   "source": [
    "## Part B: Final Data Science Project Report Assignment [30 pts]\n",
    "\n",
    "This final report is the culmination of your semester-long Data Science project, building upon the exploratory analyses and modeling milestones you've already completed. Your report should clearly communicate your findings, analysis approach, and conclusions to a technical audience. The following structure and guidelines, informed by best practices, will help you prepare a professional and comprehensive document.\n",
    "\n",
    "### Required Sections\n",
    "\n",
    "Your report must include the following sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d6069-cf27-4312-8e4b-8c27fa6cf9d0",
   "metadata": {},
   "source": [
    "#### 1. Executive Summary (Abstract) [2 pts]\n",
    "- Brief overview of the entire project (150–200 words)\n",
    "- Clearly state the objective, approach, and key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bc7a8-eac5-4dba-9052-d3fbf29adaf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5edd854f-857f-41a2-983a-845c99a87153",
   "metadata": {},
   "source": [
    "#### 2. Introduction [2 pts]\n",
    "- Clearly introduce the topic and context of your project\n",
    "- Describe the problem you are addressing (the problem statement)\n",
    "- Clearly state the objectives and goals of your analysis\n",
    "\n",
    "Note: You may imaginatively consider this project as taking place in a real estate company with a small data science group in-house, and write your introduction from this point of view (don't worry about verisimilitude to an actual company!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf823c-453c-40ba-89f0-07826b9adf7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6394185-1992-469c-b475-05bd473327b1",
   "metadata": {},
   "source": [
    "#### 3. Data Description [2 pts]\n",
    "- Describe the source of your dataset (described in Milestone 1)\n",
    "- Clearly state the characteristics of your data (size, types of features, missing values, target, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47de29-d135-4312-a46b-97427c8b1ca4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db7d906a-bd9a-42f8-8eb4-7bba1b1b108a",
   "metadata": {},
   "source": [
    "#### 4. Methodology (What you did, and why)  [12 pts]\n",
    "\n",
    "**Focus this section entirely on the steps you took and your reasoning behind them. Emphasize the process and decision-making, not the results themselves**\n",
    "\n",
    "- Describe your analytical framework \n",
    "  - Use of validation curves to see the effect of various hyperparameter choices, and\n",
    "  - Choice of RMSE as primary error metric\n",
    "- Clearly outline your data cleaning and preprocessing steps\n",
    "  - Describe what issues you encountered in the raw data and how you addressed them.\n",
    "  - Mention any key decisions (e.g., removing samples with too many missing values).\n",
    "  - What worked and what didn't work?\n",
    "- Describe your feature engineering approach\n",
    "  - Explain any transformations, combinations, or derived features.\n",
    "  - Discuss why certain features were chosen or created, even if they were later discarded.\n",
    "  - What worked and what didn't work?\n",
    "- Detail your model selection process \n",
    "  - Outline the models you experimented with and why.\n",
    "  - Discuss how you evaluated generalization (e.g., cross-validation, shape and relationships of plots).\n",
    "  - Mention how you tuned hyperparameters or selected the final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276de7c1-71ab-4bca-b1d4-e78979cbd0b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4305c55a-370f-4083-8cb6-395545ff1013",
   "metadata": {},
   "source": [
    "#### 5. Results and Evaluation (What you found, and how well it worked) [10 pts]\n",
    "\n",
    "**Focus purely on outcomes, with metrics, visuals, and insights. This is where you present evidence to support your conclusions.**\n",
    "\n",
    "- Provide a clear and detailed narrative of your analysis and reasoning using the analytical approach described in (4). \n",
    "- Discuss model performance metrics and results (RMSE, R2, etc.)\n",
    "- **Include relevant visualizations (graphs, charts, tables) with appropriate labels and captions**\n",
    "- Error analysis\n",
    "  - Highlight specific patterns of error, outliers, or questionable features.\n",
    "  - Note anything surprising or worth improving in future iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615213f-7689-47fe-ac5a-24a767faaae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97153d2-e099-4c15-99f8-ad0b6a539d4b",
   "metadata": {},
   "source": [
    "#### 6. Conclusion [2 pts]\n",
    "- Clearly state your main findings and how they address your original objectives\n",
    "- Highlight the business or practical implications of your findings \n",
    "- Discuss the limitations and constraints of your analysis clearly and transparently\n",
    "- Suggest potential improvements or future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287f955-7ae0-41ec-864d-44765c60ffe7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
