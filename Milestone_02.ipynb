{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on April 13 (with 2-hour grace period) and worth 25 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1. You will:\n",
    "\n",
    "1. Evaluate baseline models using default settings.\n",
    "2. Engineer new features and re-evaluate models.\n",
    "3. Use feature selection techniques to find promising subsets.\n",
    "4. Select the top 3 models and fine-tune them for optimal performance.\n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling.\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will not use the testing set during this milestone — it’s reserved for final evaluation later.\n",
    "- You will have to redo the scaling step when you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for reading the dataset and splitting into training and testing sets with scaled and unscaled data\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # check if the dataset has been loaded properly\n",
    "    if df.empty:\n",
    "        raise ValueError(\"the dataset is empty, please check the path or the file content.\")\n",
    "    print(f\"loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "    # Split the data into features and target variables\n",
    "    X = df.drop(columns=['taxvaluedollarcnt'])\n",
    "    y = df['taxvaluedollarcnt']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # create scaled versions of the train and test features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset with shape: (71217, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled = load_data('Milestone Data/zillow_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Baseline Modeling [3 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters**:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each model:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV RMSE Score** across all folds in a table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X_train, y_train, n_repeats=5, n_jobs=-1, **model_params):\n",
    "\n",
    "    # Instantiate the model if a class is provided, so for example can use either BaggingRegressor or BaggingRegressor() as argument. \n",
    "    if isinstance(model, type):\n",
    "        model = model(**model_params)\n",
    "\n",
    "    neg_rmse_scores = cross_val_score(model, X_train, y_train, scoring = 'neg_root_mean_squared_error',\n",
    "                                     cv = RepeatedKFold(n_splits=5, n_repeats=n_repeats, random_state=random_state), n_jobs = n_jobs)\n",
    "    \n",
    "    mean_cv_rmse = -np.mean(neg_rmse_scores)\n",
    "    std_cv_rmse  = np.std(neg_rmse_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute training RMSE\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_rmse = root_mean_squared_error(y_train, train_preds)\n",
    "    \n",
    "    return mean_cv_rmse, std_cv_rmse, train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Bagging\": BaggingRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that runs all the models and returns a Dataframe with the results\n",
    "def evaluate_model(dict):\n",
    "    results = {}\n",
    "    for model_name, model in dict.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        mean_cv_rmse, std_cv_rmse, train_rmse = run_model(model, X_train_scaled, y_train)\n",
    "        results[model_name] = {\n",
    "            'Mean CV RMSE': dollar_format(mean_cv_rmse, 2),\n",
    "            'STD CV RMSE': dollar_format(std_cv_rmse, 2),\n",
    "            'Train CV RMSE': dollar_format(train_rmse, 2)\n",
    "        }\n",
    "    \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.index.name = 'Model'\n",
    "    results_df = results_df.sort_values(by='Mean CV RMSE', ascending=True)\n",
    "    print(\"Evaluation complete. Results:\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Linear...\n",
      "Evaluating Ridge...\n",
      "Evaluating Lasso...\n",
      "Evaluating Decision Tree...\n",
      "Evaluating Bagging...\n",
      "Evaluating Random Forest...\n",
      "Evaluating Gradient Boosting...\n",
      "Evaluation complete. Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean CV RMSE</th>\n",
       "      <th>STD CV RMSE</th>\n",
       "      <th>Train CV RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>$241,624</td>\n",
       "      <td>$2,553</td>\n",
       "      <td>$91,824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>$245,820</td>\n",
       "      <td>$2,739</td>\n",
       "      <td>$242,356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>$251,688</td>\n",
       "      <td>$2,394</td>\n",
       "      <td>$107,195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>$276,212</td>\n",
       "      <td>$2,985</td>\n",
       "      <td>$276,019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>$276,212</td>\n",
       "      <td>$2,985</td>\n",
       "      <td>$276,019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>$276,212</td>\n",
       "      <td>$2,985</td>\n",
       "      <td>$276,019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>$332,940</td>\n",
       "      <td>$3,260</td>\n",
       "      <td>$24,216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Mean CV RMSE STD CV RMSE Train CV RMSE\n",
       "Model                                                   \n",
       "Random Forest         $241,624      $2,553       $91,824\n",
       "Gradient Boosting     $245,820      $2,739      $242,356\n",
       "Bagging               $251,688      $2,394      $107,195\n",
       "Linear                $276,212      $2,985      $276,019\n",
       "Ridge                 $276,212      $2,985      $276,019\n",
       "Lasso                 $276,212      $2,985      $276,019\n",
       "Decision Tree         $332,940      $3,260       $24,216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [2 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which models perform best overall?\n",
    "  - Which are most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The top 3 best performing models were the Random Forest, Gradient Boosting, and Bagging tree models. Random Forest was best overall with top 2 marks in all measurements with the best Mean CV RMSE of $241,624 \n",
    "* The top 3 most stable models are the same top 3 performers, Bagging, Random Forest, Gradient Boosting. Bagging was the most stable with a standard deviation of $2,394\n",
    "* The Decision Tree regressor might have been subject to overfitting as it's training rmse was very low, almost to the point of an optimized model, and it's Mean CV RMSE was very high compared to the rest. The Linear, Ridge, and Lasso models seemed to underfit the model because their training rmse values were almost identical to their Mean CV rmse scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [3 pts]\n",
    "\n",
    "Consider **at least three new features** based on your Milestone 1, Part 5. Examples include:\n",
    "- Polynomial terms\n",
    "- Log or interaction terms\n",
    "- Groupings or transformations of categorical features\n",
    "\n",
    "Add these features to `X_train` and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run all models listed above (using default settings again).\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table. \n",
    "\n",
    "**Note:**  Recall that this will require creating a new version of the dataset, so effectively you may be running \"polynomial regression\" using `LinearRegression`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [2 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "- Were there any unexpected results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [3 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features.\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [2 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n",
    "\n",
    "- How did feature selection differ between linear and tree-based models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Top 3 Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far.\n",
    "\n",
    "1. Choose the top 3 models based on performance and interpretability from earlier parts.\n",
    "2. For each model:\n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, or other techniques from previous homeworks. \n",
    "   - Experiment with different versions of your feature engineering and preprocessing — treat these as additional tunable components.\n",
    "3. Report the mean and standard deviation of CV RMSE score for each model in a summary table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [4 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n",
    "- Provide a ranking of your three models and explain your reasoning — not just based on RMSE, but also interpretability, training time, or generalizability.\n",
    "- Conclude by considering whether this workflow has produced the results you expected. Typically, you would repeat steps 2 - 4 and also reconsider the choices you made in Milestone 1 when cleaning the dataset, until reaching the point of diminishing returns; do you think that would that have helped here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
